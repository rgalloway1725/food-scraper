{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook for scraping serious eats articles, and using machine learning to determine the author\\\n",
    "Rob Galloway\\\n",
    "2-1-20\n",
    "\n",
    "Code flow:\n",
    "- Use BeautifulSoup to scrape articles from seriouseats.com\n",
    "- Convert articles to word frequency lists (bag of words)\n",
    "- Train and test machine learning classifier to identify author using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing our packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests # For getting url's\n",
    "import numpy as np # For data analysis/manipulation\n",
    "from bs4 import BeautifulSoup # Web scraper\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Converts strings to word frequency arrays\n",
    "from sklearn import neighbors # KNN machine learning package\n",
    "from sklearn.svm import SVC # SVC ML package\n",
    "from sklearn.tree import DecisionTreeClassifier # Decision Tree ML package\n",
    "from sklearn.ensemble import RandomForestClassifier # Random Forest ML package\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping\n",
    "- Go to (https://www.seriouseats.com/how-tos/cooking-techniques) and use BeautifulSoup to grab links (iterate through every page)\n",
    "\n",
    "- Follow links to articles, use BS to scrape articles for text\n",
    "- Store each article as a single string, corpus variable is a list of article strings\n",
    "\n",
    "NOTE: This cell takes a long time to run as it is scraping and storing >200 articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "corpus = []\n",
    "authors = []\n",
    "\n",
    "# This loop iterates through each page\n",
    "for page in range(1,13):\n",
    "    \n",
    "    # concatenate url string with page numbers and convert to soup\n",
    "    result = requests.get('https://www.seriouseats.com/how-tos/cooking-techniques?page='+str(page)+\\\n",
    "                          '&sort=latest#recipes')\n",
    "    src = result.content\n",
    "    soup = BeautifulSoup(src, \"lxml\") # This is the searchable soup\n",
    "    \n",
    "    # Isolate section of page that contains the article links\n",
    "    block = soup.find('div', class_='block__wrapper')\n",
    "    \n",
    "    \n",
    "    # Loop through every post on the page\n",
    "    # scrape the article link and follow it to the article\n",
    "    # scrape the text in each article and store to corpus\n",
    "    for div_tag in block.find_all('div', class_='metadata'):\n",
    "        \n",
    "        link = div_tag.find('a').attrs['href'] # this line scrapes and saves the link        \n",
    "        \n",
    "        # scrape text and author from link and store\n",
    "        newresult = requests.get(link)\n",
    "        newsrc = newresult.content\n",
    "        newsoup = BeautifulSoup(newsrc, \"lxml\") # This is the searchable soup\n",
    "        \n",
    "        # scrape the author's name and store\n",
    "        author = newsoup.find('a', class_='name').get_text()\n",
    "        authors = authors+[author] # This is the list of authors\n",
    "        \n",
    "        # Isolate main body from soup\n",
    "        newbody = newsoup.find('div', class_='entry-body')\n",
    "        \n",
    "        \n",
    "        text = '' # reset text variable before next loop\n",
    "        \n",
    "        # Scrape text and combine\n",
    "        for p_tag in newbody.find_all('p'):\n",
    "            \n",
    "            text = text+p_tag.get_text() # combine text blocks\n",
    "        \n",
    "        # Add essay text to corpus list\n",
    "        corpus= corpus+[text] # This is where all of the text is stored\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Formatting\n",
    "\n",
    "- Convert raw strings to word frequency arrays (Bag Of Words)\n",
    "- SKlearn has built in CountVectorizer package (easy!)\n",
    "- Authors left as strings for easy analysis later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer() # Call the vectorizer package using default settings\n",
    "X = vectorizer.fit_transform(corpus) # Convert corpus to word frequency vector\n",
    "freq = X.toarray() # Store as array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "\n",
    "ML classifiers come in many different flavors:\n",
    "- K Nearest Neighbors\n",
    "- Support Vector Classifiers\n",
    "- Decision Tree/Random Forest\n",
    "\n",
    "<img src=\"classifiers.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28000000000000003"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a K Nearest Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(3, weights='distance') # Define classifier\n",
    "clf.fit(freq[50:,:], authors[50:]) # Train on a portion of the data\n",
    "\n",
    "# Test on the remaining data, this will output the percent accuracy on the test data\n",
    "clf.score(freq[:50], authors[:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea what this number represents, we can have the classifier output the prediced author of some articles and compare them to the actual authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['J. Kenji L贸pez-Alt' 'J. Kenji L贸pez-Alt' 'Niki Achitoff-Gray'\n",
      " 'Stella Parks']\n",
      "['Stella Parks', 'Sasha Marx', 'The Serious Eats Team', 'Stella Parks']\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(freq[6:10])) # Predict the authors of articles 6 through 9\n",
    "print(authors[6:10]) # Print the actual authors of articles 6 through 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47999999999999998"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = SVC(kernel='linear',gamma='auto')\n",
    "clf2.fit(freq[50:,:], authors[50:]) # Train on a portion of the data\n",
    "\n",
    "# Test on the remaining data, this will output the percent accuracy on the test data\n",
    "clf2.score(freq[:50], authors[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stella Parks' 'Daniel Gritzer' 'Daniel Gritzer' 'Stella Parks']\n",
      "['Stella Parks', 'Sasha Marx', 'The Serious Eats Team', 'Stella Parks']\n"
     ]
    }
   ],
   "source": [
    "print(clf2.predict(freq[6:10])) # Predict the authors of articles 6 through 9\n",
    "print(authors[6:10]) # Print the actual authors of articles 6 through 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41999999999999998"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3 = DecisionTreeClassifier(random_state=1)\n",
    "clf3.fit(freq[50:,:], authors[50:]) # Train on a portion of the data\n",
    "\n",
    "# Test on the remaining data, this will output the percent accuracy on the test data\n",
    "clf3.score(freq[:50], authors[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stella Parks' 'J. Kenji L贸pez-Alt' 'J. Kenji L贸pez-Alt' 'Stella Parks']\n",
      "['Stella Parks', 'Sasha Marx', 'The Serious Eats Team', 'Stella Parks']\n"
     ]
    }
   ],
   "source": [
    "print(clf3.predict(freq[6:10])) # Predict the authors of articles 6 through 9\n",
    "print(authors[6:10]) # Print the actual authors of articles 6 through 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35999999999999999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4 = RandomForestClassifier(random_state=1,n_estimators=10)\n",
    "clf4.fit(freq[50:,:], authors[50:]) # Train on a portion of the data\n",
    "\n",
    "# Test on the remaining data, this will output the percent accuracy on the test data\n",
    "clf4.score(freq[:50], authors[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stella Parks' 'J. Kenji L贸pez-Alt' 'J. Kenji L贸pez-Alt' 'Stella Parks']\n",
      "['Stella Parks', 'Sasha Marx', 'The Serious Eats Team', 'Stella Parks']\n"
     ]
    }
   ],
   "source": [
    "print(clf4.predict(freq[6:10])) # Predict the authors of articles 6 through 9\n",
    "print(authors[6:10]) # Print the actual authors of articles 6 through 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "- Not enough data\n",
    "- Vocabulary too large\n",
    "- Model not behaving as intended\n",
    "\n",
    "https://youtu.be/cjvS2bB0UVg?t=79\n",
    "(2:55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['J. Kenji L贸pez-Alt' 'J. Kenji L贸pez-Alt' 'J. Kenji L贸pez-Alt'\n",
      " 'J. Kenji L贸pez-Alt' 'J. Kenji L贸pez-Alt' 'J. Kenji L贸pez-Alt'\n",
      " 'Stella Parks' 'Daniel Gritzer' 'Daniel Gritzer' 'Stella Parks'\n",
      " 'J. Kenji L贸pez-Alt' 'J. Kenji L贸pez-Alt' 'J. Kenji L贸pez-Alt'\n",
      " 'J. Kenji L贸pez-Alt' 'J. Kenji L贸pez-Alt' 'J. Kenji L贸pez-Alt'\n",
      " 'J. Kenji L贸pez-Alt' 'J. Kenji L贸pez-Alt' 'Stella Parks' 'Daniel Gritzer']\n"
     ]
    }
   ],
   "source": [
    "print(clf2.predict(freq[:20]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
